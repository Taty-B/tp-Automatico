"""
EXTRACCI√ìN DE EMBEDDINGS BERT PARA TP2 - AA2
============================================

Este script extrae los embeddings de BERT multilingual para los tokens
de los datasets tokenizados (train, val, test) y los guarda en formato
numpy para usar en las redes neuronales.

Basado en el c√≥digo de ejemplo de la consigna del TP.
"""

import pandas as pd
import numpy as np
import torch
from transformers import BertTokenizer, BertModel
import os
from tqdm import tqdm
import pickle

def setup_bert_model():
    """
    Configura el modelo BERT multilingual y tokenizer
    """
    print("ü§ñ CONFIGURANDO MODELO BERT MULTILINGUAL")
    print("=" * 50)
    
    model_name = "bert-base-multilingual-cased"
    
    tokenizer = BertTokenizer.from_pretrained(model_name)
    model = BertModel.from_pretrained(model_name)
    model.eval()  # Modo evaluaci√≥n
    
    print(f"‚úÖ Modelo cargado: {model_name}")
    print(f"‚úÖ Vocabulario size: {tokenizer.vocab_size}")
    print(f"‚úÖ Embedding dimension: {model.config.hidden_size}")
    
    return tokenizer, model

def get_token_embedding(token, tokenizer, model):
    """
    Extrae el embedding est√°tico para un token espec√≠fico
    Basado en el c√≥digo de ejemplo de la consigna
    """
    # Validar que el token sea string
    if not isinstance(token, str) or pd.isna(token):
        token = "[UNK]"  # Usar token desconocido si no es v√°lido
    
    # Convertir token a ID
    token_id = tokenizer.convert_tokens_to_ids(token)
    
    # Verificar si el token existe en el vocabulario
    if token_id is None or token_id == tokenizer.unk_token_id:
        token_id = tokenizer.unk_token_id
    
    # Extraer embedding del modelo
    with torch.no_grad():
        embedding_vector = model.embeddings.word_embeddings.weight[token_id]
    
    return embedding_vector.detach().numpy()

def process_tokenized_dataset(file_path, tokenizer, model, dataset_name):
    """
    Procesa un dataset tokenizado y extrae embeddings para todos los tokens
    """
    print(f"\nüìä PROCESANDO DATASET: {dataset_name.upper()}")
    print("=" * 50)
    
    # Cargar dataset tokenizado
    print("üìÇ Cargando dataset...")
    df = pd.read_csv(file_path)
    print(f"‚úÖ Dataset cargado: {len(df):,} tokens")
    
    # Extraer tokens √∫nicos para optimizar
    print("üîç Analizando tokens √∫nicos...")
    
    # Limpiar tokens inv√°lidos
    df['token'] = df['token'].fillna('[UNK]')  # Reemplazar NaN con [UNK]
    df['token'] = df['token'].astype(str)      # Asegurar que sean strings
    
    unique_tokens = df['token'].unique()
    print(f"üìù Tokens √∫nicos encontrados: {len(unique_tokens):,}")
    
    # Verificar si hay tokens problem√°ticos
    invalid_tokens = [t for t in unique_tokens if not isinstance(t, str) or pd.isna(t)]
    if invalid_tokens:
        print(f"‚ö†Ô∏è  Tokens inv√°lidos encontrados: {len(invalid_tokens)} (ser√°n reemplazados por [UNK])")
    
    # Crear diccionario de embeddings para tokens √∫nicos
    print(f"\nüß† Extrayendo embeddings de {len(unique_tokens):,} tokens √∫nicos...")
    token_embeddings = {}
    
    # Barra de progreso detallada para embeddings
    pbar_embeddings = tqdm(
        unique_tokens, 
        desc=f"üîÑ Embeddings {dataset_name}",
        unit="tokens",
        bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
    )
    
    for token in pbar_embeddings:
        embedding = get_token_embedding(token, tokenizer, model)
        token_embeddings[token] = embedding
        # Actualizar descripci√≥n con progreso
        pbar_embeddings.set_postfix({
            'completado': f"{len(token_embeddings)}/{len(unique_tokens)}",
            'dim': '768'
        })
    
    print(f"‚úÖ Embeddings extra√≠dos para {len(token_embeddings):,} tokens √∫nicos")
    
    # Crear matriz de embeddings para todo el dataset
    print(f"\nüîß Construyendo matriz de features para {len(df):,} tokens...")
    embeddings_matrix = []
    labels_punt_inicial = []
    labels_punt_final = []
    labels_capitalizacion = []
    
    # Barra de progreso detallada para construcci√≥n de matriz
    pbar_matrix = tqdm(
        df.iterrows(), 
        total=len(df),
        desc=f"üèóÔ∏è  Matriz {dataset_name}",
        unit="tokens",
        bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
    )
    
    for idx, row in pbar_matrix:
        token = row['token']
        embedding = token_embeddings[token]
        embeddings_matrix.append(embedding)
        
        # Procesar labels
        labels_punt_inicial.append(1 if row['punt_inicial'] == '¬ø' else 0)
        
        # Labels puntuaci√≥n final
        punt_final = row['punt_final']
        if punt_final == ',':
            labels_punt_final.append(0)
        elif punt_final == '.':
            labels_punt_final.append(1)
        elif punt_final == '?':
            labels_punt_final.append(2)
        else:  # sin puntuaci√≥n
            labels_punt_final.append(3)
        
        # Labels capitalizaci√≥n (ya est√°n como n√∫meros)
        labels_capitalizacion.append(int(row['capitalizacion']))
        
        # Actualizar cada 1000 tokens para no saturar
        if idx % 1000 == 0:
            pbar_matrix.set_postfix({
                'procesados': f"{idx+1}/{len(df)}",
                'shape': f"({len(embeddings_matrix)}, 768)"
            })
    
    print(f"‚úÖ Matriz de features construida: {len(embeddings_matrix):,} √ó 768")
    
    # Convertir a arrays numpy
    print("üîÑ Convirtiendo a arrays numpy...")
    X = np.array(embeddings_matrix)
    y_punt_inicial = np.array(labels_punt_inicial)
    y_punt_final = np.array(labels_punt_final)
    y_capitalizacion = np.array(labels_capitalizacion)
    
    print(f"‚úÖ Features extra√≠das:")
    print(f"   X shape: {X.shape}")
    print(f"   y_punt_inicial shape: {y_punt_inicial.shape}")
    print(f"   y_punt_final shape: {y_punt_final.shape}")
    print(f"   y_capitalizacion shape: {y_capitalizacion.shape}")
    
    # Estad√≠sticas de labels
    print(f"\nüìà Distribuci√≥n de labels:")
    print(f"   Punt. inicial - ¬ø: {(y_punt_inicial == 1).sum():,} ({(y_punt_inicial == 1).mean()*100:.1f}%)")
    print(f"   Punt. inicial - sin: {(y_punt_inicial == 0).sum():,} ({(y_punt_inicial == 0).mean()*100:.1f}%)")
    
    print(f"   Punt. final - coma: {(y_punt_final == 0).sum():,} ({(y_punt_final == 0).mean()*100:.1f}%)")
    print(f"   Punt. final - punto: {(y_punt_final == 1).sum():,} ({(y_punt_final == 1).mean()*100:.1f}%)")
    print(f"   Punt. final - ?: {(y_punt_final == 2).sum():,} ({(y_punt_final == 2).mean()*100:.1f}%)")
    print(f"   Punt. final - sin: {(y_punt_final == 3).sum():,} ({(y_punt_final == 3).mean()*100:.1f}%)")
    
    for i in range(4):
        count = (y_capitalizacion == i).sum()
        pct = (y_capitalizacion == i).mean() * 100
        print(f"   Capitalizaci√≥n {i}: {count:,} ({pct:.1f}%)")
    
    return X, (y_punt_inicial, y_punt_final, y_capitalizacion), df

def save_features(X, y_tuple, df, dataset_name, output_dir="bert_features"):
    """
    Guarda las features y labels extra√≠das
    """
    print(f"\nüíæ GUARDANDO FEATURES: {dataset_name.upper()}")
    print("=" * 50)
    
    # Crear directorio si no existe
    os.makedirs(output_dir, exist_ok=True)
    
    # Guardar features (embeddings)
    X_file = os.path.join(output_dir, f"X_{dataset_name}.npy")
    np.save(X_file, X)
    print(f"‚úÖ Features guardadas: {X_file}")
    
    # Guardar labels
    y_punt_inicial, y_punt_final, y_capitalizacion = y_tuple
    
    y_punt_inicial_file = os.path.join(output_dir, f"y_punt_inicial_{dataset_name}.npy")
    y_punt_final_file = os.path.join(output_dir, f"y_punt_final_{dataset_name}.npy")
    y_capitalizacion_file = os.path.join(output_dir, f"y_capitalizacion_{dataset_name}.npy")
    
    np.save(y_punt_inicial_file, y_punt_inicial)
    np.save(y_punt_final_file, y_punt_final)
    np.save(y_capitalizacion_file, y_capitalizacion)
    
    print(f"‚úÖ Labels guardadas:")
    print(f"   {y_punt_inicial_file}")
    print(f"   {y_punt_final_file}")
    print(f"   {y_capitalizacion_file}")
    
    # Guardar informaci√≥n adicional
    info = {
        'dataset_name': dataset_name,
        'num_samples': len(X),
        'embedding_dim': X.shape[1],
        'num_instances': df['instancia_id'].nunique(),
        'label_distributions': {
            'punt_inicial': {
                'sin_puntuacion': int((y_punt_inicial == 0).sum()),
                'pregunta_apertura': int((y_punt_inicial == 1).sum())
            },
            'punt_final': {
                'coma': int((y_punt_final == 0).sum()),
                'punto': int((y_punt_final == 1).sum()),
                'pregunta': int((y_punt_final == 2).sum()),
                'sin_puntuacion': int((y_punt_final == 3).sum())
            },
            'capitalizacion': {
                'minusculas': int((y_capitalizacion == 0).sum()),
                'title_case': int((y_capitalizacion == 1).sum()),
                'mixed_case': int((y_capitalizacion == 2).sum()),
                'uppercase': int((y_capitalizacion == 3).sum())
            }
        }
    }
    
    info_file = os.path.join(output_dir, f"info_{dataset_name}.pkl")
    with open(info_file, 'wb') as f:
        pickle.dump(info, f)
    print(f"‚úÖ Info guardada: {info_file}")

def main():
    """
    Funci√≥n principal para extraer embeddings de todos los datasets
    """
    print("üöÄ EXTRACTOR DE EMBEDDINGS BERT MULTILINGUAL")
    print("=" * 60)
    
    # Configurar modelo BERT
    print("‚öôÔ∏è  Inicializando modelo BERT...")
    tokenizer, model = setup_bert_model()
    
    # Datasets a procesar
    datasets = [
        ("tokenized_train.csv", "train"),
        ("tokenized_val.csv", "val"),
        ("tokenized_test.csv", "test")
    ]
    
    print(f"\nüìã PLAN DE PROCESAMIENTO:")
    print(f"   Datasets a procesar: {len(datasets)}")
    for i, (file_path, name) in enumerate(datasets, 1):
        print(f"   {i}. {name.upper()} ({file_path})")
    
    # Procesar cada dataset con barra de progreso general
    print(f"\nüîÑ INICIANDO PROCESAMIENTO DE DATASETS...")
    
    pbar_general = tqdm(
        datasets,
        desc="üìä Datasets",
        unit="dataset",
        bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]"
    )
    
    for file_path, dataset_name in pbar_general:
        pbar_general.set_description(f"üìä Procesando {dataset_name.upper()}")
        pbar_general.set_postfix({'actual': dataset_name})
        
        print(f"\n{'='*60}")
        X, y_tuple, df = process_tokenized_dataset(file_path, tokenizer, model, dataset_name)
        save_features(X, y_tuple, df, dataset_name)
        print(f"‚úÖ {dataset_name.upper()} procesado exitosamente")
    
    print(f"\nüéâ EXTRACCI√ìN DE EMBEDDINGS COMPLETADA")
    print("=" * 60)
    print("üìÅ Los archivos se guardaron en el directorio 'bert_features/'")
    print("üîÑ Siguiente paso: Entrenar las redes neuronales con neural_networks.py")

if __name__ == "__main__":
    main()

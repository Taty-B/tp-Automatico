# ğŸ§  PLAN: RED NEURONAL BIDIRECCIONAL - TP2 AA2

## ğŸ¯ OBJETIVO PRINCIPAL
Implementar una **RNN bidireccional** que prediga simultÃ¡neamente:
1. **PuntuaciÃ³n inicial** (Â¿ o sin puntuaciÃ³n)
2. **PuntuaciÃ³n final** (coma, punto, ? o sin puntuaciÃ³n) 
3. **CapitalizaciÃ³n** (0=minus, 1=title, 2=mixed, 3=upper)

---

## ğŸ“‹ REQUISITOS ESTRICTOS DE LA CONSIGNA

### âœ… **Arquitectura Obligatoria**
- [x] **RNN bidireccional** (forward + backward)
- [x] Puede usar **LSTM** o **GRU** como celdas
- [x] **Multi-tarea**: 3 salidas simultÃ¡neas
- [x] Entrada: embeddings de `bert-base-multilingual-cased` (768 dim)

### âœ… **Datos de Entrada**
- [x] Tokens procesados con tokenizador BERT multilingual
- [x] Embeddings estÃ¡ticos de BERT (no contextuales)
- [x] Predicciones a **nivel de token** (no palabra)

### âœ… **Tareas y Labels**
1. **PuntuaciÃ³n inicial**: 
   - `0` = sin puntuaciÃ³n
   - `1` = Â¿ (signo de pregunta apertura)

2. **PuntuaciÃ³n final**:
   - `0` = coma (,)
   - `1` = punto (.)
   - `2` = signo pregunta cierre (?)
   - `3` = sin puntuaciÃ³n

3. **CapitalizaciÃ³n**:
   - `0` = todo minÃºsculas ("hola")
   - `1` = primera mayÃºscula ("Hola") 
   - `2` = mixta ("McDonald's", "iPhone")
   - `3` = todo mayÃºsculas ("UBA", "NASA")

### âœ… **EvaluaciÃ³n**
- **MÃ©trica**: F1 macro para cada tarea
- **Nivel**: EvaluaciÃ³n a nivel palabra (no token)
- **Clases**:
  - Punt. inicial: 2 clases (Â¿, sin)
  - Punt. final: 4 clases (coma, punto, ?, sin)
  - CapitalizaciÃ³n: 4 clases (0, 1, 2, 3)

---

## ğŸ—ï¸ ARQUITECTURA DETALLADA

### **1. Capa de Entrada**
```python
# Input shape: (batch_size, sequence_length, 768)
# 768 = dimensiÃ³n embeddings BERT
input_layer = Input(shape=(None, 768))
```

### **2. Capa Bidireccional**
```python
# Bidirectional LSTM/GRU
# return_sequences=True para predicciÃ³n por token
bidirectional_layer = Bidirectional(
    LSTM(units=128, return_sequences=True),
    merge_mode='concat'  # concat forward + backward
)
# Output shape: (batch_size, sequence_length, 256)
```

### **3. Capas de Salida Multi-tarea**
```python
# 3 cabezas independientes para cada tarea
punct_initial_head = Dense(2, activation='softmax', name='punct_initial')
punct_final_head = Dense(4, activation='softmax', name='punct_final') 
capitalization_head = Dense(4, activation='softmax', name='capitalization')
```

### **4. FunciÃ³n de PÃ©rdida Multi-tarea**
```python
losses = {
    'punct_initial': 'sparse_categorical_crossentropy',
    'punct_final': 'sparse_categorical_crossentropy',
    'capitalization': 'sparse_categorical_crossentropy'
}

# Pesos balanceados segÃºn distribuciÃ³n de clases
loss_weights = {
    'punct_initial': 1.0,
    'punct_final': 1.0, 
    'capitalization': 2.0  # MÃ¡s peso por desbalance
}
```

---

## ğŸ“Š MANEJO DE SECUENCIAS

### **Problema: Secuencias de Longitud Variable**
- Los textos tienen diferentes longitudes
- BERT tokeniza en sub-tokens (##)
- Necesitamos padding/truncation

### **SoluciÃ³n: Estrategia de Secuencias**
1. **Chunking**: Dividir secuencias muy largas
2. **Padding**: Rellenar secuencias cortas
3. **Masking**: Ignorar tokens de padding en pÃ©rdida

```python
# ParÃ¡metros de secuencia
MAX_SEQUENCE_LENGTH = 512  # LÃ­mite BERT
PADDING_VALUE = 0
```

---

## ğŸ›ï¸ HIPERPARÃMETROS INICIALES

### **Arquitectura**
- **Unidades LSTM**: 128 (bidireccional â†’ 256 total)
- **Dropout**: 0.3 (prevenir overfitting)
- **Recurrent dropout**: 0.2

### **Entrenamiento**
- **Batch size**: 32 (ajustar segÃºn memoria)
- **Learning rate**: 0.001 (Adam optimizer)
- **Epochs**: 50 (con early stopping)
- **Validation split**: Usar val set existente

### **Callbacks**
- **EarlyStopping**: patience=5, monitor='val_loss'
- **ModelCheckpoint**: guardar mejor modelo
- **ReduceLROnPlateau**: reducir LR si no mejora

---

## ğŸ”„ PIPELINE DE ENTRENAMIENTO

### **Paso 1: Carga de Datos**
```python
# Cargar embeddings y labels
X_train = np.load('bert_features/X_train.npy')
y_punct_initial_train = np.load('bert_features/y_punt_inicial_train.npy')
y_punct_final_train = np.load('bert_features/y_punt_final_train.npy')
y_capitalization_train = np.load('bert_features/y_capitalizacion_train.npy')
```

### **Paso 2: PreparaciÃ³n de Secuencias**
```python
# Agrupar tokens por instancia_id
# Crear secuencias de longitud variable
# Aplicar padding/truncation
```

### **Paso 3: ConstrucciÃ³n del Modelo**
```python
# Definir arquitectura bidireccional
# Compilar con optimizador y mÃ©tricas
# Mostrar resumen del modelo
```

### **Paso 4: Entrenamiento**
```python
# Fit con validation data
# Callbacks para monitoreo
# Guardar mÃ©tricas de entrenamiento
```

### **Paso 5: EvaluaciÃ³n**
```python
# Predicciones en test set
# CÃ¡lculo de F1 macro por tarea
# Matriz de confusiÃ³n
# AnÃ¡lisis de errores
```

---

## âš ï¸ REGLAS Y CONSIDERACIONES CRÃTICAS

### **ğŸš¨ REGLA 1: Manejo de Secuencias**
- **NUNCA** mezclar tokens de diferentes instancias
- **SIEMPRE** mantener correspondencia token â†” label
- **CUIDADO** con el padding: usar masking en la pÃ©rdida

### **ğŸš¨ REGLA 2: Multi-tarea**
- **3 salidas simultÃ¡neas** (no modelos separados)
- **Pesos de pÃ©rdida** balanceados por desbalance de clases
- **MÃ©tricas independientes** para cada tarea

### **ğŸš¨ REGLA 3: EvaluaciÃ³n Correcta**
- **F1 macro** (no weighted, no micro)
- **Por tarea separada** (3 mÃ©tricas F1)
- **Nivel palabra** en evaluaciÃ³n final (aunque entrenemos por token)

### **ğŸš¨ REGLA 4: Reproducibilidad**
- **Seeds fijos** para numpy, tensorflow, random
- **Guardar configuraciÃ³n** completa del modelo
- **Logs detallados** de entrenamiento

### **ğŸš¨ REGLA 5: Manejo de Memoria**
- **Batch size adaptativo** segÃºn GPU disponible
- **Gradient checkpointing** si es necesario
- **LiberaciÃ³n de memoria** entre pasos

---

## ğŸ“ˆ MÃ‰TRICAS Y MONITOREO

### **Durante Entrenamiento**
- Loss total y por tarea
- Accuracy por tarea
- Validation metrics
- Learning rate actual

### **EvaluaciÃ³n Final**
- **F1 macro** por tarea (MÃ‰TRICA PRINCIPAL)
- Precision y Recall por clase
- Matriz de confusiÃ³n
- AnÃ¡lisis de casos difÃ­ciles

---

## ğŸš€ PLAN DE IMPLEMENTACIÃ“N

### **Fase 1: PreparaciÃ³n de Datos** âœ…
- [x] Cargar embeddings BERT
- [x] Verificar shapes y tipos
- [ ] Agrupar por secuencias (instancia_id)
- [ ] Implementar padding/truncation

### **Fase 2: Arquitectura**
- [ ] Definir modelo bidireccional
- [ ] Implementar multi-tarea
- [ ] Configurar pÃ©rdidas y mÃ©tricas

### **Fase 3: Entrenamiento**
- [ ] Pipeline de entrenamiento
- [ ] Callbacks y monitoreo
- [ ] ValidaciÃ³n en val set

### **Fase 4: EvaluaciÃ³n**
- [ ] Predicciones en test
- [ ] CÃ¡lculo F1 macro
- [ ] AnÃ¡lisis de resultados

### **Fase 5: OptimizaciÃ³n**
- [ ] Tuning de hiperparÃ¡metros
- [ ] AnÃ¡lisis de errores
- [ ] Mejoras de arquitectura

---

## ğŸ¯ CRITERIOS DE Ã‰XITO

### **MÃ­nimo Aceptable**
- Modelo entrena sin errores
- F1 > 0.5 en las 3 tareas
- Predicciones coherentes

### **Objetivo Deseable**
- F1 > 0.7 en puntuaciÃ³n
- F1 > 0.8 en capitalizaciÃ³n
- Convergencia estable

### **Excelencia**
- F1 > 0.85 en todas las tareas
- AnÃ¡lisis detallado de errores
- ComparaciÃ³n con baselines

---

## ğŸ“ ESTRUCTURA DE ARCHIVOS

```
neural_networks.py          # ImplementaciÃ³n principal
models/                     # Modelos guardados
â”œâ”€â”€ bidirectional_model.h5
â”œâ”€â”€ model_config.json
â””â”€â”€ training_history.pkl
results/                    # Resultados y mÃ©tricas
â”œâ”€â”€ training_curves.png
â”œâ”€â”€ confusion_matrices.png
â”œâ”€â”€ f1_scores.json
â””â”€â”€ error_analysis.txt
```

---

## ğŸ”§ DEBUGGING Y TROUBLESHOOTING

### **Problemas Comunes**
1. **OOM (Out of Memory)**: Reducir batch_size
2. **Nan Loss**: Verificar learning rate, gradient clipping
3. **No convergencia**: Ajustar arquitectura, datos
4. **Overfitting**: MÃ¡s dropout, regularizaciÃ³n

### **Verificaciones CrÃ­ticas**
- [ ] Shapes de entrada y salida
- [ ] Correspondencia token-label
- [ ] DistribuciÃ³n de clases balanceada
- [ ] MÃ©tricas calculadas correctamente

---

## ğŸ“ NOTAS FINALES

- **Seguir consigna AL PIE DE LA LETRA**
- **Documentar todas las decisiones**
- **CÃ³digo limpio y comentado**
- **Resultados reproducibles**

Â¡VAMOS A CREAR UNA RED BIDIRECCIONAL Ã‰PICA! ğŸš€ğŸ§  